{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from trial_class import *\n",
    "from experiment_class import Experiment\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tdt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from scipy.signal import butter, filtfilt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sp_extension import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_path = r\"C:\\Users\\alber\\OneDrive\\Desktop\\PC_Lab\\Photometry\\Pilot_2\\Combined_Cohorts\\Social_Pref\\nac\"\n",
    "csv_base_path = r\"C:\\Users\\alber\\OneDrive\\Desktop\\PC_Lab\\Photometry\\Pilot_2\\Combined_Cohorts\\Social_Pref\\nac_csvs\"\n",
    "\n",
    "# NAc: #15616F\n",
    "# mPFC: #FFAF00\n",
    "\n",
    "cups = r\"C:\\Users\\alber\\OneDrive\\Desktop\\PC_Lab\\Photometry\\Pilot_2\\Combined_Cohorts\\Social_Pref\\Social_Pref_sheet.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Synapse note file: C:\\Users\\alber\\OneDrive\\Desktop\\PC_Lab\\Photometry\\Pilot_2\\Combined_Cohorts\\Social_Pref\\nac\\n1-240522-072114\\Notes.txt\n",
      "read from t=0s to t=794.67s\n",
      "Found Synapse note file: C:\\Users\\alber\\OneDrive\\Desktop\\PC_Lab\\Photometry\\Pilot_2\\Combined_Cohorts\\Social_Pref\\nac\\n2-240522-084131\\Notes.txt\n",
      "read from t=0s to t=789.95s\n",
      "Found Synapse note file: C:\\Users\\alber\\OneDrive\\Desktop\\PC_Lab\\Photometry\\Pilot_2\\Combined_Cohorts\\Social_Pref\\nac\\n3-240523-073132\\Notes.txt\n",
      "read from t=0s to t=788.57s\n",
      "Found Synapse note file: C:\\Users\\alber\\OneDrive\\Desktop\\PC_Lab\\Photometry\\Pilot_2\\Combined_Cohorts\\Social_Pref\\nac\\n4-240523-084829\\Notes.txt\n",
      "read from t=0s to t=790.88s\n",
      "Found Synapse note file: C:\\Users\\alber\\OneDrive\\Desktop\\PC_Lab\\Photometry\\Pilot_2\\Combined_Cohorts\\Social_Pref\\nac\\n5-240826-083822\\Notes.txt\n",
      "read from t=0s to t=793.05s\n",
      "Found Synapse note file: C:\\Users\\alber\\OneDrive\\Desktop\\PC_Lab\\Photometry\\Pilot_2\\Combined_Cohorts\\Social_Pref\\nac\\n6-240826-094701\\Notes.txt\n",
      "read from t=0s to t=800.05s\n",
      "Found Synapse note file: C:\\Users\\alber\\OneDrive\\Desktop\\PC_Lab\\Photometry\\Pilot_2\\Combined_Cohorts\\Social_Pref\\nac\\n7-240827-072608\\Notes.txt\n",
      "read from t=0s to t=795.64s\n",
      "Processing n1-240522-072114...\n",
      "Processing n2-240522-084131...\n",
      "Processing n3-240523-073132...\n",
      "Processing n4-240523-084829...\n",
      "Processing n5-240826-083822...\n",
      "Processing n6-240826-094701...\n",
      "Processing n7-240827-072608...\n"
     ]
    }
   ],
   "source": [
    "# groups csv + experiment data into one variable\n",
    "experiment = Experiment(experiment_path, csv_base_path)\n",
    "\n",
    "# batch process the data, removing the specified time segments for subjects\n",
    "experiment.default_batch_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing behaviors for n1-240522-072114...\n",
      "Processing behaviors for n2-240522-084131...\n",
      "Processing behaviors for n3-240523-073132...\n",
      "Processing behaviors for n4-240523-084829...\n",
      "Processing behaviors for n5-240826-083822...\n",
      "Processing behaviors for n6-240826-094701...\n",
      "Processing behaviors for n7-240827-072608...\n"
     ]
    }
   ],
   "source": [
    "bout_definitions = [\n",
    "    {'prefix': 'Subject', 'introduced': 'Subject Introduced', 'removed': 'Subject Removed'},\n",
    "]\n",
    "\n",
    "\n",
    "experiment.group_extract_manual_annotations(bout_definitions,first_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dopamine Stuff (Need help with this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_combined_da_metrics(experiment, sniff_cup_csv_path, metric_list=None, first_only=False):\n",
    "    import pandas as pd\n",
    "    import re\n",
    "\n",
    "    # Normalize behavior label spacing\n",
    "    def normalize_behavior_label(label):\n",
    "        return re.sub(r'\\s+', ' ', label.strip().lower().replace('\\u00a0', ' '))\n",
    "\n",
    "    assign_df = pd.read_csv(sniff_cup_csv_path)\n",
    "    assign_df['Subject'] = assign_df['Subject'].astype(str).str.lower()\n",
    "\n",
    "    # Build subject -> behavior name -> agent identity mapping\n",
    "    subject_to_behavior_to_agent = {}\n",
    "    for _, row in assign_df.iterrows():\n",
    "        subj = row['Subject']\n",
    "        subject_to_behavior_to_agent[subj] = {}\n",
    "        for col in row.index:\n",
    "            col_norm = normalize_behavior_label(str(col))\n",
    "            if col_norm.startswith(\"sniff cup\"):\n",
    "                agent_label = normalize_behavior_label(str(row[col]))\n",
    "                subject_to_behavior_to_agent[subj][col_norm] = agent_label\n",
    "\n",
    "    all_rows = []\n",
    "\n",
    "    for trial_name, trial in experiment.trials.items():\n",
    "        if not hasattr(trial, 'behaviors') or trial.behaviors.empty:\n",
    "            continue\n",
    "\n",
    "        df = trial.behaviors.copy()\n",
    "        df['Behavior'] = df['Behavior'].astype(str).apply(normalize_behavior_label)\n",
    "\n",
    "        subject_id = trial_name.lower()\n",
    "\n",
    "        if subject_id not in subject_to_behavior_to_agent:\n",
    "            continue\n",
    "\n",
    "        mapping = subject_to_behavior_to_agent[subject_id]\n",
    "\n",
    "        # Keep only sniff cup behaviors\n",
    "        df = df[df[\"Behavior\"].str.startswith(\"sniff cup\")]\n",
    "\n",
    "        # Map behaviors to agents\n",
    "        df[\"Agent\"] = df[\"Behavior\"].apply(lambda b: mapping.get(b))\n",
    "        df[\"Subject\"] = subject_id\n",
    "        df[\"Trial\"] = trial_name\n",
    "\n",
    "        unmatched = df[df[\"Agent\"].isna()]\n",
    "        if not unmatched.empty:\n",
    "            print(f\"‼️ Unmatched behaviors for subject '{subject_id}':\")\n",
    "            print(\"Behaviors that failed to map:\", unmatched[\"Behavior\"].unique())\n",
    "            print(\"Available mapping keys:\", list(mapping.keys()))\n",
    "\n",
    "        df = df.dropna(subset=[\"Agent\"])\n",
    "\n",
    "        # Choose metrics\n",
    "        known_cols = [\"Behavior\", \"Agent\", \"Subject\", \"Trial\"]\n",
    "        if metric_list:\n",
    "            metric_cols = [m for m in metric_list if m in df.columns]\n",
    "        else:\n",
    "            metric_cols = [c for c in df.columns if c not in known_cols and pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "        if not metric_cols:\n",
    "            continue\n",
    "\n",
    "        df = df[[\"Subject\", \"Agent\"] + metric_cols]\n",
    "\n",
    "        if first_only:\n",
    "            df = df.groupby([\"Subject\", \"Agent\"], as_index=False).first()\n",
    "\n",
    "        all_rows.append(df)\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"⚠️ No rows added to DataFrame. Check if behavior labels match and mapping keys are clean.\")\n",
    "        print(f\"Subjects in experiment: {list(experiment.trials.keys())}\")\n",
    "        print(f\"Subjects in assignments file: {assign_df['Subject'].tolist()}\")\n",
    "        print(\"Sample mapping dictionary:\")\n",
    "        for subj, mapping in subject_to_behavior_to_agent.items():\n",
    "            print(f\"{subj} -> {mapping}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined_df = pd.concat(all_rows, ignore_index=True)\n",
    "\n",
    "    # --- Aggregate by Subject-Agent pair ---\n",
    "    if first_only:\n",
    "        grouped = combined_df  # already one row per subject-agent\n",
    "    else:\n",
    "        grouped = combined_df.groupby([\"Subject\", \"Agent\"], as_index=False)[metric_cols].mean()\n",
    "\n",
    "    # --- Ensure each subject has all 4 agent rows ---\n",
    "    all_agents = ['nothing', 'short_term', 'long_term', 'novel']\n",
    "    all_subjects = sorted(grouped['Subject'].unique())\n",
    "    full_index = pd.MultiIndex.from_product([all_subjects, all_agents], names=['Subject', 'Agent'])\n",
    "\n",
    "    final_df = (\n",
    "        grouped.set_index(['Subject', 'Agent'])\n",
    "               .reindex(full_index)\n",
    "               .fillna(0)\n",
    "               .reset_index()\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Final DA metrics DataFrame created with {len(final_df)} rows from {len(all_subjects)} subjects.\")\n",
    "    return final_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing DA metrics for n1-240522-072114 ...\n",
      "Computing DA metrics for n2-240522-084131 ...\n",
      "Computing DA metrics for n3-240523-073132 ...\n",
      "Computing DA metrics for n4-240523-084829 ...\n",
      "Computing DA metrics for n5-240826-083822 ...\n",
      "Computing DA metrics for n6-240826-094701 ...\n",
      "Computing DA metrics for n7-240827-072608 ...\n"
     ]
    }
   ],
   "source": [
    "experiment.compute_all_da_metrics(use_max_length=False,\n",
    "                                  max_bout_duration=5, #total_avg_bout_duration\n",
    "                                  mode='standard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# --- helpers ---------------------------------------------------------------\n",
    "\n",
    "def _norm(s: str) -> str:\n",
    "    \"\"\"lowercase, collapse whitespace, replace NBSPs\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", str(s).strip().lower().replace(\"\\u00a0\", \" \"))\n",
    "\n",
    "def _canonical_agent(s: str) -> str:\n",
    "    \"\"\"map common variants to canonical tokens used in your code\"\"\"\n",
    "    s = _norm(s).replace(\"-\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    alias = {\n",
    "        \"empty\": \"nothing\",\n",
    "        \"short_term\": \"short_term\",\n",
    "        \"long_term\": \"long_term\",\n",
    "        \"novel\": \"novel\",\n",
    "        \"nothing\": \"nothing\"\n",
    "    }\n",
    "    return alias.get(s, s)\n",
    "\n",
    "def _build_agent_mapping(assign_csv: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns long-form mapping with columns: subject, cupnum (Int64), agent\n",
    "    \"\"\"\n",
    "    map_df = pd.read_csv(assign_csv)\n",
    "    map_df.columns = [_norm(c) for c in map_df.columns]\n",
    "    map_df[\"subject\"] = map_df[\"subject\"].astype(str).str.lower()\n",
    "\n",
    "    cup_cols = [c for c in map_df.columns if c.startswith(\"sniff cup\")]\n",
    "    long = map_df.melt(\n",
    "        id_vars=[\"subject\"],\n",
    "        value_vars=cup_cols,\n",
    "        var_name=\"cup_label\",\n",
    "        value_name=\"agent\"\n",
    "    )\n",
    "    long[\"cupnum\"] = long[\"cup_label\"].str.extract(r\"(\\d+)\").astype(\"Int64\")\n",
    "    long[\"agent\"] = long[\"agent\"].map(_canonical_agent)\n",
    "    return long.dropna(subset=[\"cupnum\"])[[\"subject\", \"cupnum\", \"agent\"]]\n",
    "\n",
    "\n",
    "def _collect_sniff_events(experiment, metric_cols=None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Concatenate all blocks' sniff-cup events with requested metrics.\n",
    "    Returns columns: Subject, cupnum, Event_Start (if present), <metric_cols...>\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for block_name, block in experiment.trials.items():  # 'trials' container\n",
    "        if not hasattr(block, \"behaviors\") or block.behaviors.empty:\n",
    "            continue\n",
    "\n",
    "        df = block.behaviors.copy()\n",
    "        if \"Behavior\" not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # normalize and keep only 'sniff cup n' rows\n",
    "        beh = df[\"Behavior\"].astype(str).map(_norm)\n",
    "        mask = beh.str.match(r\"^sniff\\s*cup\\s*\\d+$\")\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        df = df.loc[mask].copy()\n",
    "        df[\"cupnum\"] = beh.loc[mask].str.extract(r\"(\\d+)\").astype(\"Int64\")\n",
    "        df[\"Subject\"] = str(block_name).lower()\n",
    "\n",
    "        # pick metrics (default: all numeric except bookkeeping)\n",
    "        if metric_cols is None:\n",
    "            numeric_cols = df.select_dtypes(include=\"number\").columns.tolist()\n",
    "            # keep common metrics; drop obvious non-metrics if present\n",
    "            drop = {\"event_end\", \"adjusted end\", \"time of max peak\"}\n",
    "            metric_cols_use = [c for c in numeric_cols if _norm(c) not in drop]\n",
    "        else:\n",
    "            metric_cols_use = [c for c in metric_cols if c in df.columns]\n",
    "\n",
    "        keep = [\"Subject\", \"cupnum\"] + ([\"Event_Start\"] if \"Event_Start\" in df.columns else []) + metric_cols_use\n",
    "        frames.append(df[keep])\n",
    "\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame(\n",
    "        columns=[\"Subject\", \"cupnum\"] + (metric_cols or [])\n",
    "    )\n",
    "\n",
    "# --- main API --------------------------------------------------------------\n",
    "\n",
    "def associate_da_with_agents(\n",
    "    experiment,\n",
    "    assign_csv: str,\n",
    "    metric_cols=None,          # e.g., [\"Max Peak\", \"Mean Z-score\", \"AUC\"]\n",
    "    first_only: bool = True,   # first event per Subject×Agent (by Event_Start)\n",
    "    ensure_all_agents=False,   # reindex to nothing/short_term/long_term/novel\n",
    "    fill_value=np.nan\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Produces a tidy dataframe with DA metrics per Subject×Agent efficiently.\n",
    "    \"\"\"\n",
    "    # gather events and build mapping\n",
    "    events = _collect_sniff_events(experiment, metric_cols=metric_cols)\n",
    "    if events.empty:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    mapping = _build_agent_mapping(assign_csv)\n",
    "\n",
    "    # merge on Subject + cup number to attach Agent\n",
    "    merged = events.merge(\n",
    "        mapping, how=\"left\",\n",
    "        left_on=[\"Subject\", \"cupnum\"],\n",
    "        right_on=[\"subject\", \"cupnum\"]\n",
    "    ).drop(columns=[\"subject\"])\n",
    "\n",
    "    merged.rename(columns={\"agent\": \"Agent\"}, inplace=True)\n",
    "\n",
    "    # choose first event or aggregate mean per Subject×Agent\n",
    "    if first_only and \"Event_Start\" in merged.columns:\n",
    "        merged = (merged.sort_values([\"Subject\", \"Agent\", \"Event_Start\"])\n",
    "                        .drop_duplicates(subset=[\"Subject\", \"Agent\"], keep=\"first\"))\n",
    "    else:\n",
    "        agg = {c: \"mean\" for c in merged.columns if c not in [\"Subject\", \"Agent\", \"cupnum\", \"Event_Start\"]}\n",
    "        merged = merged.groupby([\"Subject\", \"Agent\"], as_index=False).agg(agg)\n",
    "\n",
    "    # optional: ensure each subject has all 4 agents\n",
    "    if ensure_all_agents:\n",
    "        agents = [\"nothing\", \"short_term\", \"long_term\", \"novel\"]\n",
    "        merged[\"Agent\"] = merged[\"Agent\"].map(_canonical_agent)\n",
    "        idx = pd.MultiIndex.from_product([sorted(merged[\"Subject\"].unique()), agents],\n",
    "                                         names=[\"Subject\", \"Agent\"])\n",
    "        merged = (merged.set_index([\"Subject\", \"Agent\"])\n",
    "                        .reindex(idx)\n",
    "                        .reset_index()\n",
    "                        .fillna(fill_value))\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The column label 'cupnum' is not unique.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28788\\712665138.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m associate_da_with_agents(experiment=experiment,\n\u001b[0m\u001b[0;32m      2\u001b[0m                          \u001b[0massign_csv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcups\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28788\\3573613634.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(experiment, assign_csv, metric_cols, first_only, ensure_all_agents, fill_value)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[0mmapping\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_build_agent_mapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_csv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;31m# merge on Subject + cup number to attach Agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m     merged = events.merge(\n\u001b[0m\u001b[0;32m    104\u001b[0m         \u001b[0mmapping\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"left\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Subject\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cupnum\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"subject\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"cupnum\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\photometry\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10828\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mMergeValidate\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10829\u001b[0m     \u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10830\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m> 10832\u001b[1;33m         return merge(\n\u001b[0m\u001b[0;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m  10835\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\photometry\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m         op = _MergeOperation(\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\photometry\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 794\u001b[1;33m         \u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_merge_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    795\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    796\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\photometry\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1306\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlk\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m                         \u001b[1;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m                         \u001b[1;31m#  the latter of which will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m                         \u001b[0mlk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m                         \u001b[0mleft_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m                         \u001b[0mjoin_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m                         \u001b[1;31m# work-around for merge_asof(left_index=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\alber\\anaconda3\\envs\\photometry\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1921\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1922\u001b[0m                 \u001b[0mmulti_message\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1924\u001b[0m             \u001b[0mlabel_axis_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"column\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"index\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1925\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   1926\u001b[0m                 \u001b[1;33mf\"\u001b[0m\u001b[1;33mThe \u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mlabel_axis_name\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m label '\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m' is not unique.\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmulti_message\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1927\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The column label 'cupnum' is not unique."
     ]
    }
   ],
   "source": [
    "associate_da_with_agents(experiment=experiment,\n",
    "                         assign_csv=cups)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "photometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
